{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd # to load the datasets\n",
    "# to preprocess the data\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "# to analyze the data\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "# to extract features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# to build the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "train_data = pd.read_csv('train_E6oV3lV.csv')\n",
    "test_data = pd.read_csv('test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    '''\n",
    "        Class to pre-process the data.\n",
    "        \n",
    "        Functions:\n",
    "        \n",
    "        remove_user_handle - to remove the user handle or other patterns from the tweet text.\n",
    "        \n",
    "        remove_unnecessary_chars - to remove numbers, punctuations and any special characters from the data.\n",
    "        \n",
    "        rm_stopwords_stem - to remove the stop words and performing stemming to remove the suffix from the words.\n",
    "    ''' \n",
    "    def __init__(self):\n",
    "        self.operators = {'and','or','not'}\n",
    "        self.stop_words = set(stopwords.words('english'))-self.operators\n",
    "\n",
    "    def remove_user_handle(self, raw_data, pattern):\n",
    "        r = re.findall(pattern, raw_data)\n",
    "        for i  in r:\n",
    "            raw_data = re.sub(i,'', raw_data)\n",
    "        return raw_data\n",
    "    \n",
    "    def remove_unnecessary_chars(self, data):\n",
    "        data['clean_tweet'] = data['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        return data\n",
    "    \n",
    "    def rm_stopwords_stem(self, data):\n",
    "        data['clean_tweet'] = data['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "        split_sent = data['clean_tweet'].apply(lambda x: x.split())\n",
    "        filtered_sentence = split_sent.apply(lambda tweet: [ps.stem(word) for word in tweet]) #if not word in self.stop_words]) \n",
    "        for i in range(len(filtered_sentence)):\n",
    "            filtered_sentence[i] =' '.join(filtered_sentence[i])\n",
    "        data['clean_tweet'] = filtered_sentence\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analyzing_data:\n",
    "    '''\n",
    "        Class to analyze the data.\n",
    "        \n",
    "        Functions:\n",
    "        \n",
    "        understand_word_sentim - to remove the user handle or other patterns from the tweet text.\n",
    "        \n",
    "        check_hashtags - to remove numbers, punctuations and any special characters from the data.\n",
    "        \n",
    "        distribution_top_N_words - to remove the stop words and performing stemming to remove the suffix from the words.\n",
    "    ''' \n",
    "    def understand_word_sentim(self, data, label):\n",
    "        list_of_words = ' '.join([text for text in data['clean_tweet'][data['label'] == label]])\n",
    "        wordcloud = WordCloud(height = 700, width = 1000, random_state  = 21, max_font_size = 30).generate(list_of_words)\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def check_hashtags(self, data, label):\n",
    "        hashtags = []\n",
    "        data = data['clean_tweet'][data['label']==label]\n",
    "        for row in data:\n",
    "            ht = re.findall(r'#(\\w+)', row)\n",
    "            hashtags.extend(ht)\n",
    "        return hashtags\n",
    "    \n",
    "    def distribution_top_N_words(self, hashtags, N):\n",
    "        freq = nltk.FreqDist(hashtags)\n",
    "        df = pd.DataFrame({'Hashtags': list(freq.keys()),\n",
    "                     'count': list(freq.values())})\n",
    "        df = df.nlargest(N, 'count', keep='first')\n",
    "        plt.figure(figsize=(16, 5))\n",
    "        ax = sns.barplot(data = df, y='count', x = 'Hashtags')\n",
    "        plt.xlabel('Top 10 words')\n",
    "        plt.ylabel('count')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_extraction:\n",
    "    '''\n",
    "        Class to apply feature extraction.\n",
    "        \n",
    "        Functions:\n",
    "        \n",
    "        extract_using_BOW - converting text into features using Bag of words.\n",
    "        \n",
    "        extract_using_tfidf - converting text into features using TF-IDF.\n",
    "    ''' \n",
    "    def extract_using_BOW(self, data):\n",
    "        cv = CountVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "        X = cv.fit_transform(data['clean_tweet'])\n",
    "        return X\n",
    "    \n",
    "    def extract_using_tfidf(self, data):\n",
    "        tf = TfidfVectorizer(max_df = 0.90, min_df = 2, max_features = 1000, stop_words='english')\n",
    "        X = tf.fit_transform(data['clean_tweet'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    '''\n",
    "        Function to split the dataset into training and validation data\n",
    "    '''\n",
    "    # Separate data into training and validation\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(dataset, train_data['label'], test_size = 0.20, random_state = 42)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Models:\n",
    "    def LRModel(self, X_train, Y_train):\n",
    "        classifier_LR = LogisticRegression(solver='liblinear', random_state = 0)\n",
    "        classifier_LR.fit(X_train, Y_train)\n",
    "        return classifier_LR\n",
    "    \n",
    "    def SVMModel(self, X_train, Y_train):\n",
    "        classifier_SVM = SVC(kernel = 'linear', C = 1, gamma = 'auto', probability = True, random_state = 0)\n",
    "        classifier_SVM.fit(X_train, Y_train)\n",
    "        return classifier_SVM\n",
    "    \n",
    "#     def SVMModel_rbf(self, X_train, Y_train):\n",
    "#         classifier_SVM = SVC(kernel = 'rbf', C = 1, gamma = 'auto', probability = True, random_state = 0)\n",
    "#         classifier_SVM.fit(X_train, Y_train)\n",
    "#         return classifier_SVM\n",
    "    \n",
    "    def DTModel(self, X_train, Y_train):\n",
    "        classifier_DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "        classifier_DT.fit(X_train, Y_train)\n",
    "        return classifier_DT\n",
    "    \n",
    "#     def DTModel_gini(self, X_train, Y_train):\n",
    "#         classifier_DT = DecisionTreeClassifier(criterion = 'gini', random_state = 0) \n",
    "#         classifier_DT.fit(X_train, Y_train)\n",
    "#         return classifier_DT\n",
    "    \n",
    "    def RFModel(self, X_train, Y_train):\n",
    "        classifier_RF = RandomForestClassifier(n_estimators = 1000, criterion = 'entropy', random_state = 0)\n",
    "        classifier_RF.fit(X_train, Y_train)\n",
    "        return classifier_RF\n",
    "    \n",
    "    def NBModel(self, X_train, Y_train):\n",
    "        classifier_NB = MultinomialNB(alpha = 1.0)\n",
    "        classifier_NB.fit(X_train, Y_train)\n",
    "        return classifier_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1score(Y_test, Y_pred, classifier):\n",
    "    '''\n",
    "        Function to calculate the F1-score on the validation dataset.\n",
    "    '''\n",
    "    score = f1_score(Y_test, Y_pred)\n",
    "    print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "    print('F1 score of classifier ', classifier, ' is: ', score)\n",
    "    print('\\n')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predictions(classifier, X_test):\n",
    "    '''\n",
    "        Function to predict the outcome based on the trained model.\n",
    "    '''\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    y_pred_int = y_pred[:,1] >= 0.3\n",
    "    y_pred_int = y_pred_int.astype(np.int)\n",
    "    return y_pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_config(model, parameters, X_train, Y_train):\n",
    "    print('Grid Search for ', model)\n",
    "    grid_search = GridSearchCV(estimator = model, \n",
    "                               param_grid = parameters, \n",
    "                               scoring = 'f1', \n",
    "                               cv = 5, \n",
    "                               n_jobs = -1)\n",
    "    grid_search = grid_search.fit(X_train, Y_train)\n",
    "    return [str(grid_search.best_params_), grid_search.best_estimator_, grid_search.best_score_]\n",
    "\n",
    "def candidate_param_list():\n",
    "    model_param_list = []\n",
    "    \n",
    "    SVM_params = [{'C': [1,10,100,1000], 'kernel' : ['linear']},\n",
    "                 {'C':[1,2,10,100], 'kernel' : ['rbf'], 'gamma': [0.5, 0.6, 0.7, 0.8]}]\n",
    "    model_param_list.append([\"SVM\", SVM_params])\n",
    "    \n",
    "    RF_params = [{'n_estimators' : [10, 100, 200, 250, 400], 'criterion' : ['entropy','gini']}]\n",
    "    model_param_list.append([\"RF\", RF_params])\n",
    "    \n",
    "    DT_params = [{'criterion': ['entropy', 'gini']}]\n",
    "    model_param_list.append([\"DT\", DT_params])\n",
    "    \n",
    "    LR_params = [{'solver': ['liblinear']}]\n",
    "    model_param_list.append([\"LR\", LR_params])\n",
    "    \n",
    "    NB_params = [{'alpha': [0.2,1.0]}]\n",
    "    model_param_list.append([\"NB\", NB_params])\n",
    "    \n",
    "    return model_param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison_grid_search(model_param_list, X_train, Y_train):\n",
    "    '''\n",
    "        Function for model comparison.\n",
    "        This function returns the model with the greatest F1-score.\n",
    "    '''\n",
    "    max_f1score = 0\n",
    "    best_classifier = None\n",
    "    classifier_grid = []\n",
    "    cm = Classification_Models()\n",
    "    LR = cm.LRModel(X_train, Y_train)\n",
    "    SVM = cm.SVMModel(X_train, Y_train)\n",
    "    DT = cm.DTModel(X_train, Y_train)\n",
    "    RF = cm.RFModel(X_train, Y_train)\n",
    "    NB = cm.NBModel(X_train, Y_train)\n",
    "\n",
    "    for model, parameters in model_param_list:\n",
    "        print(model)\n",
    "        print(eval(model))\n",
    "        classifier_grid.append(best_config(eval(model), parameters, X_train, Y_train))\n",
    "        print(classifier_grid)\n",
    "        \n",
    "    for name, classifier, score in classifier_grid:\n",
    "        if max_f1score < score:\n",
    "            max_f1score = score\n",
    "            best_classifier = classifier\n",
    "    return best_classifier, max_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search parameters\n",
    "model_param_list = candidate_param_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False)\n",
      "Grid Search for  SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False)\n",
      "[[\"{'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\", SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False), 0.615446286744192]]\n",
      "RF\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "Grid Search for  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "[[\"{'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\", SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False), 0.615446286744192], [\"{'criterion': 'entropy', 'n_estimators': 400}\", RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False), 0.5862005319713827]]\n",
      "DT\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best')\n",
      "Grid Search for  DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best')\n",
      "[[\"{'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\", SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False), 0.615446286744192], [\"{'criterion': 'entropy', 'n_estimators': 400}\", RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False), 0.5862005319713827], [\"{'criterion': 'entropy'}\", DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best'), 0.541382274819738]]\n",
      "LR\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Grid Search for  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "[[\"{'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\", SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False), 0.615446286744192], [\"{'criterion': 'entropy', 'n_estimators': 400}\", RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False), 0.5862005319713827], [\"{'criterion': 'entropy'}\", DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best'), 0.541382274819738], [\"{'solver': 'liblinear'}\", LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 0.47999846109773286]]\n",
      "NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Grid Search for  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAMAN\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"{'C': 10, 'gamma': 0.5, 'kernel': 'rbf'}\", SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False), 0.615446286744192], [\"{'criterion': 'entropy', 'n_estimators': 400}\", RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False), 0.5862005319713827], [\"{'criterion': 'entropy'}\", DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best'), 0.541382274819738], [\"{'solver': 'liblinear'}\", LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 0.47999846109773286], [\"{'alpha': 0}\", MultinomialNB(alpha=0, class_prior=None, fit_prior=True), 0.4648449492022663]]\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the training and test data\n",
    "p = preprocess()\n",
    "train_data['clean_tweet'] = np.vectorize(p.remove_user_handle)(train_data['tweet'], \"@[\\w]*\")\n",
    "p.remove_unnecessary_chars(train_data)\n",
    "p.rm_stopwords_stem(train_data)\n",
    "\n",
    "test_data['clean_tweet'] = np.vectorize(p.remove_user_handle)(test_data['tweet'], \"@[\\w]*\")\n",
    "p.remove_unnecessary_chars(test_data)\n",
    "p.rm_stopwords_stem(test_data)\n",
    "\n",
    "Analyzing training data\n",
    "a = analyzing_data()\n",
    "# positive sentiments\n",
    "try:\n",
    "    print('Positive/Neutral sentiments')\n",
    "    a.understand_word_sentim(train_data, 0) \n",
    "    pos_hashtags = a.check_hashtags(train_data, 0)\n",
    "    a.distribution_top_N_words(pos_hashtags, 10)\n",
    "except:\n",
    "    print(sys.exc_info()[0],\": \", sys.exc_info()[1])\n",
    "# negative sentiments\n",
    "try:\n",
    "    print('Negative sentiments')\n",
    "    a.understand_word_sentim(train_data, 1) \n",
    "    neg_hashtags = a.check_hashtags(train_data, 1)\n",
    "    a.distribution_top_N_words(neg_hashtags, 10)\n",
    "except:\n",
    "    print(sys.exc_info()[0],\": \", sys.exc_info()[1])\n",
    "\n",
    "# Using Bag of Words\n",
    "# Bag of Words (Count Vectorizer)\n",
    "fe = feature_extraction()\n",
    "X_BOW = fe.extract_using_BOW(train_data)\n",
    "\n",
    "# Tfidf Vectorizer\n",
    "X_TF = fe.extract_using_tfidf(train_data)\n",
    "\n",
    "# Grid search parameters\n",
    "model_param_list = candidate_param_list()\n",
    "\n",
    "print('Using BAG OF WORDS: ')\n",
    "X_train_bow, X_test_bow, Y_train_bow, Y_test_bow = split_data(X_BOW)\n",
    "best_classifier_bw, f1_score_bw = model_comparison_grid_search(model_param_list, X_train_bow, Y_train_bow)\n",
    "\n",
    "# Using TFidf\n",
    "print('Using TF-IDF: ')\n",
    "X_train_tf, X_test_tf, Y_train_tf, Y_test_tf = split_data(X_TF)\n",
    "best_classifier_tf, f1_score_tf = model_comparison_grid_search(model_param_list, X_train_tf, Y_train_tf)\n",
    "\n",
    "if f1_score_tf > f1_score_bw:\n",
    "    print('Using TF-IDF for the final predictions.')\n",
    "#     x_test = fe.extract_using_tfidf(test_data)\n",
    "    x_test = X_test_tf\n",
    "    y_test = Y_test_tf\n",
    "    best_classifier = best_classifier_tf\n",
    "else:\n",
    "    print('Using BAG OF WORDS for the final predictions.')\n",
    "#     x_test = fe.extract_using_BOW(test_data)\n",
    "    x_test = X_test_bow\n",
    "    y_test = Y_test_bow\n",
    "    best_classifier = best_classifier_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False) 0.615446286744192\n"
     ]
    }
   ],
   "source": [
    "print(best_classifier_tf, f1_score_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9513530423901142\n",
      "F1 score of classifier  test dataset  is:  0.6221142162818954\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6221142162818954"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Predict output\n",
    "# test_data['label'] = calc_predictions(best_classifier, x_test)\n",
    "# test_data[['id', 'label']].to_csv('test_predictions.csv', index=False)\n",
    "y_predict = calc_predictions(best_classifier, x_test)\n",
    "calculate_f1score(y_test, y_predict, 'test dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
